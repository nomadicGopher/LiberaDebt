package main

import (
 "context"
 "fmt"
 "io"
 "log"
 "os"
 "time"

 "github.com/ollama/ollama"
)

func main() {
 // --- Configuration ---
 ollamaHost := "http://localhost:11434" // Your Ollama server address
 modelName := "llama3" // The model you want to use/pull
 prompt := "Tell me a short story about a brave knight and a wise dragon."

 // --- 1. Initialize Ollama Client ---
 client, err := ollama.NewClient(ollamaHost)
 if err != nil {
  log.Fatalf("Error creating Ollama client: %v", err)
 }

 // --- 2. Check for Model Presence and Pull if Missing ---
 fmt.Printf("Checking for model '%s'...\n", modelName)
 err = ensureModel(client, modelName)
 if err != nil {
  log.Fatalf("Failed to ensure model '%s': %v", modelName, err)
 }
 fmt.Printf("Model '%s' is ready.\n\n", modelName)

 // --- 3. Perform Streaming Generate Request ---
 fmt.Printf("Sending streaming generate request for model '%s'...\n", modelName)
 fmt.Println("--- Ollama Response (Streaming) ---")

 generateCtx, cancelGenerate := context.WithTimeout(context.Background(), 5*time.Minute) // Set a timeout for generation
 defer cancelGenerate()

 // Perform the generate request with Stream: true
 respFunc := func(resp *ollama.GenerateResponse) error {
  fmt.Print(resp.Response) // Print each chunk as it arrives
  return nil
 }

 err = client.Generate(generateCtx, &ollama.GenerateRequest{
  Model: modelName,
  Prompt: prompt,
  Stream: true, // Crucial for streaming
 }, respFunc) // Pass the response handler function

 if err != nil {
  // Specific error handling for context cancellation (e.g., timeout)
  if generateCtx.Err() == context.Canceled || generateCtx.Err() == context.DeadlineExceeded {
   log.Printf("\nGenerate request timed out or was cancelled: %v", generateCtx.Err())
  } else {
   log.Fatalf("\nError during streaming generation: %v", err)
  }
 }
 fmt.Println("\n--- End of Streaming Response ---\n")

 // --- Optional: Non-streaming request for comparison ---
 // fmt.Println("\n--- Non-streaming request for comparison ---")
 // nonStreamingResponse, err := client.Generate(context.Background(), &ollama.GenerateRequest{
 // Model: modelName,
 // Prompt: "What is the capital of Canada?",
 // Stream: false,
 // })
 // if err != nil {
 // log.Fatalf("Error generating non-streaming response: %v", err)
 // }
 // fmt.Printf("Non-streaming response: %s\n", nonStreamingResponse.Response)
}

// ensureModel checks if a model exists and pulls it if it doesn't.
func ensureModel(client *ollama.Client, modelName string) error {
 // List local models
 listModelsCtx, cancelList := context.WithTimeout(context.Background(), 30*time.Second)
 defer cancelList()

 listResp, err := client.List(listModelsCtx)
 if err != nil {
  return fmt.Errorf("failed to list models: %w", err)
 }

 modelExists := false
 for _, model := range listResp.Models {
  if model.Model == modelName {
   modelExists = true
   break
  }
 }

 if modelExists {
  fmt.Printf("Model '%s' already exists locally.\n", modelName)
  return nil
 }

 fmt.Printf("Model '%s' not found. Attempting to pull...\n", modelName)

 pullCtx, cancelPull := context.WithTimeout(context.Background(), 30*time.Minute) // Give ample time for large models
 defer cancelPull()

 pullFunc := func(resp *ollama.ProgressResponse) error {
  // Ollama sends various status updates during pull (download, verifying, etc.)
  // You can parse resp.Status to show more granular progress.
  // For simplicity, we'll just show status.
  fmt.Printf(" Status: %s\r", resp.Status) // \r returns cursor to start of line
  return nil
 }

 err = client.Pull(pullCtx, &ollama.PullRequest{Model: modelName}, pullFunc)
 if err != nil {
  return fmt.Errorf("failed to pull model '%s': %w", modelName, err)
 }
 fmt.Printf("\nSuccessfully pulled model '%s'.\n", modelName) // Newline after progress
 return nil
}
